# ğŸ§  Classification de tweets selon le sentiment (positif ou nÃ©gatif)

Ce projet a pour but principal  de fournir un **serveur d'infÃ©rence** permettant de prÃ©dire le **sentiment (positif ou nÃ©gatif) d'un tweet** Ã  partir de plusieurs modÃ¨les dâ€™apprentissage automatique, allant dâ€™approches classiques Ã  des modÃ¨les avancÃ©s basÃ©s sur BERT.

L'entraÃ®nement des modÃ¨les est rÃ©alisÃ© Ã  partir dâ€™un jeu de donnÃ©es annotÃ© contenant **1 600 000 tweets** (0 = NÃ©gatif, 4 = Positif), disponible sur Kaggle :  
ğŸ”— https://www.kaggle.com/datasets/kazanova/sentiment140

Pour l'entrainement, le stockage et la mesure des performances des modÃ¨les, j'utilise un serveur MLflow local, pour l'installation voir ğŸ‘‰ https://www.mlflow.org/docs/latest/ml/tracking/quickstart.

Pour faciliter la portabilitÃ© du serveur d'infÃ©rence, celui-ci sera contenu dans une **image docker**.

Deux images dockers seront construites:

 - 1. Une image lÃ©gÃ¨re utilisant un modÃ¨le BERT entrainÃ© sur les donnÃ©es via MLflow pour l'infÃ©rence, exposant une API (FastApi) et conÃ§ue pour tourner sur du CPU.
 - 2. Une image plus lourde qui inclue en plus une interface web interactive (Streamlit) qui permet Ã  un utilisateur de saisir un tweet, d'obtenir une prÃ©diction et d'envoyer un feedback qui sera remontÃ© sur une instance Azure Application Insights pour superviser les performances. 


---

## ğŸ“ Structure du projet

```bash
Classificateur-de-tweet-selon-le-sentiment-positif-ou-negatif/
â”‚
â”œâ”€â”€ notebooks/                          # ModÃ©lisation + tracking des expÃ©rimentations via MLFlow
â”œâ”€â”€ saved_model/                        # Artefacts extraits de l'entrainement du modÃ¨le BERT via le serveur MLflow qui seront utilisÃ©s pour construire le serveur d'infÃ©rence
â”œâ”€â”€ dockerfiles/                        # Fichiers utilisÃ©s pour dÃ©finir les dÃ©pendances utilisÃ©es pour les images docker ainsi que les applications Fastapi et Streamlit
â”œâ”€â”€ bert-fastapi-cpu/                   # Construction d'une image Docker avec serveur d'infÃ©rence BERT (FastAPI) optimisÃ© pour CPU
â”œâ”€â”€ bert-fastapi-streamlit-azure/       # Construction d'une image Docker avec interface web (Streamlit) + API FastAPI + logs Azure Insights
â”œâ”€â”€ scripts/                            # Scripts d'entraÃ®nement, Ã©valuation, export
```

---

## ğŸ³ Installation de Docker

Pour installer Docker sur votre machine, suivre la documentation officielle :  
ğŸ‘‰ https://docs.docker.com/engine/install/

Une fois installÃ©, vÃ©rifier que Docker fonctionne :

```bash
docker --version
```

---

## ğŸš€ Exploitation des images Docker publiques

Les images Docker du **serveur MLFlow** et du **serveur d'infÃ©rence BERT-FastAPI-CPU** sont disponibles publiquement et prÃªtes Ã  l'emploi.

### ğŸ”¹ TÃ©lÃ©charger lâ€™image d'infÃ©rence

```bash
docker pull ghcr.io/romainb35/bert-fastapi-cpu:latest
```

### ğŸ”¹ Lancer lâ€™API localement

```bash
docker run -d -p 8000:8000 ghcr.io/romainb35/bert-fastapi-cpu:latest
```

Cela lance un conteneur en arriÃ¨re-plan, accessible Ã  lâ€™adresse :  
ğŸ“ http://localhost:8000/docs (documentation Swagger de lâ€™API)

---

