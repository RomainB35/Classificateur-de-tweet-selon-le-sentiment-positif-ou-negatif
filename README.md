# ğŸ§  Classification de tweets selon le sentiment (positif ou nÃ©gatif)

Ce projet a pour but de fournir un **serveur d'infÃ©rence** permettant de prÃ©dire le **sentiment d'un tweet** Ã  partir de plusieurs modÃ¨les dâ€™apprentissage automatique, allant dâ€™approches classiques Ã  des modÃ¨les avancÃ©s basÃ©s sur BERT.

L'entraÃ®nement des modÃ¨les est rÃ©alisÃ© Ã  partir dâ€™un jeu de donnÃ©es annotÃ© contenant **1 600 000 tweets** (0 = NÃ©gatif, 4 = Positif), disponible sur Kaggle :  
ğŸ”— https://www.kaggle.com/datasets/kazanova/sentiment140

L'ensemble du projet est packagÃ© sous forme dâ€™**images Docker** pour garantir la portabilitÃ©. Il intÃ¨gre :
- Le **suivi des expÃ©rimentations** via **MLFlow**
- Une **API d'infÃ©rence** via **FastAPI**
- Une **interface utilisateur** via **Streamlit**
- Un systÃ¨me de **feedback utilisateur** connectÃ© Ã  **Azure Application Insights** pour le monitoring et la traÃ§abilitÃ©.

---

## ğŸ“ Structure du projet

```bash
Classificateur-de-tweet-selon-le-sentiment-positif-ou-negatif/
â”‚
â”œâ”€â”€ notebooks/                          # ModÃ©lisation + tracking des expÃ©rimentations via MLFlow
â”œâ”€â”€ mlflow-server/                      # Image Docker contenant un serveur MLFlow
â”œâ”€â”€ bert-fastapi-cpu/                   # Image Docker avec serveur d'infÃ©rence BERT (FastAPI) optimisÃ© pour CPU
â”œâ”€â”€ bert-fastapi-streamlit-azure/      # Image Docker avec interface web (Streamlit) + API FastAPI + logs Azure Insights
â”œâ”€â”€ scripts/                            # Scripts d'entraÃ®nement, Ã©valuation, export
```

---

## ğŸ³ Installation de Docker

Pour installer Docker sur votre machine, suivre la documentation officielle :  
ğŸ‘‰ https://docs.docker.com/engine/install/

Une fois installÃ©, vÃ©rifier que Docker fonctionne :

```bash
docker --version
```

---

## ğŸš€ Exploitation des images Docker publiques

Les images Docker du **serveur MLFlow** et du **serveur d'infÃ©rence BERT-FastAPI-CPU** sont disponibles publiquement et prÃªtes Ã  l'emploi.

### ğŸ”¹ TÃ©lÃ©charger lâ€™image d'infÃ©rence

```bash
docker pull ghcr.io/romainb35/bert-fastapi-cpu:latest
```

### ğŸ”¹ Lancer lâ€™API localement

```bash
docker run -d -p 8000:8000 ghcr.io/romainb35/bert-fastapi-cpu:latest
```

Cela lance un conteneur en arriÃ¨re-plan, accessible Ã  lâ€™adresse :  
ğŸ“ http://localhost:8000/docs (documentation Swagger de lâ€™API)

---

