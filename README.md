# ğŸ§  Classification de tweets selon le sentiment (positif ou nÃ©gatif)

Ce projet a pour but principal  de fournir un **serveur d'infÃ©rence** permettant de prÃ©dire le **sentiment (positif ou nÃ©gatif) d'un tweet (en anglais)** Ã  partir de plusieurs modÃ¨les dâ€™apprentissage automatique, allant dâ€™approches classiques Ã  des modÃ¨les avancÃ©s basÃ©s sur BERT.

L'entraÃ®nement des modÃ¨les est rÃ©alisÃ© Ã  partir dâ€™un jeu de donnÃ©es de tweets anglais annotÃ© contenant **1 600 000 tweets** (0 = NÃ©gatif, 4 = Positif), disponible sur Kaggle :  
ğŸ”— https://www.kaggle.com/datasets/kazanova/sentiment140

Pour l'entrainement, le stockage et la mesure des performances des modÃ¨les, j'utilise un serveur MLflow local, pour l'installation voir ğŸ‘‰ https://www.mlflow.org/docs/latest/ml/tracking/quickstart.

Pour faciliter la portabilitÃ© du serveur d'infÃ©rence, celui-ci sera contenu dans une **image docker**.

Deux images dockers seront construites:

 - 1. Une image lÃ©gÃ¨re utilisant un modÃ¨le BERT entrainÃ© sur les donnÃ©es via MLflow pour l'infÃ©rence, exposant une API (FastApi) et conÃ§ue pour tourner sur du CPU.
 - 2. Une image plus lourde qui inclue en plus une interface web interactive (Streamlit) qui permet Ã  un utilisateur de saisir un tweet, d'obtenir une prÃ©diction et d'envoyer un feedback qui sera remontÃ© sur une instance Azure Application Insights pour superviser les performances. 


---

## ğŸ“ Structure du projet

```bash
Classificateur-de-tweet-selon-le-sentiment-positif-ou-negatif/
â”‚
â”œâ”€â”€ notebooks/                          # ModÃ©lisation + tracking des expÃ©rimentations via MLFlow
â”œâ”€â”€ saved_model/                        # Artefacts extraits de l'entrainement du modÃ¨le BERT via le serveur MLflow qui seront utilisÃ©s pour construire le serveur d'infÃ©rence
â”œâ”€â”€ dockerfiles/                        # Fichiers utilisÃ©s pour construire les images docker et dÃ©finir les dÃ©pendances utilisÃ©es ainsi que les applications Fastapi et Streamlit
â”œâ”€â”€ deploy_on_cloud/                    # Scripts utilisÃ©s pour dÃ©ployer le serveur d'infÃ©rence sur google cloud

```
## ğŸ³ Installation de Docker

Pour installer Docker sur votre machine, suivre la documentation officielle :  
ğŸ‘‰ https://docs.docker.com/engine/install/

Une fois installÃ©, vÃ©rifier que Docker fonctionne :

```bash
docker --version
```

---
## Pour build les images Docker

```bash
 docker build -t bert-fastapi-cpu -f dockerfiles/bert-fastapi-cpu/Dockerfile .
 docker build -t bert-fastapi-streamlit-azure -f bert-fastapi-streamlit-azure/Dockerfile .
```

## ğŸš€ Exploitation de l'image Docker publique

L'image docker lÃ©gÃ¨re qui contient uniquement le serveur d'infÃ©rence exposÃ© via API est disponible publiquement et prÃªte Ã  l'emploi.

Cette image est aussi dÃ©ployÃ©e sur google cloud.

### ğŸ”¹ TÃ©lÃ©charger lâ€™image d'infÃ©rence

```bash
docker pull ghcr.io/romainb35/bert-fastapi-cpu:latest
```

### ğŸ”¹ Lancer lâ€™API localement

```bash
docker run -d -p 8000:8000 ghcr.io/romainb35/bert-fastapi-cpu:latest
```

Cela lance un conteneur en arriÃ¨re-plan, accessible Ã  lâ€™adresse :  
ğŸ“ http://localhost:8000/docs (documentation Swagger de lâ€™API)

---

# ğŸ§ª Tester l'API BERT FastAPI dÃ©ployÃ©e sur Cloud Run

Vous pouvez tester les diffÃ©rents endpoints de l'API avec `curl`. Assurez-vous d'avoir `jq` installÃ© pour un affichage lisible des rÃ©ponses JSON.

La premiÃ¨re requÃªte prend plus de temps Ã  s'Ã©xÃ©cuter si le container n'est pas dÃ©jÃ  dÃ©marrÃ©.

La documentation de l'API est disponible ici ğŸ‘‰ https://bert-fastapi-service-70236624058.europe-west1.run.app/docs .

---

## ğŸ” 1. Tester le endpoint racine (`/`)

Ce endpoint permet simplement de vÃ©rifier que l'API est opÃ©rationnelle.

```bash
curl -X GET https://bert-fastapi-service-70236624058.europe-west1.run.app/ | jq
```

RÃ©ponse attendue :
```bash
{
  "message": "Service BERT FastAPI is running."
}
```

## ğŸ’¬ 2. PrÃ©diction sur un tweet unique (/predict)

Ce endpoint renvoie le sentiment d'un tweet (positif ou nÃ©gatif).

```bash
curl -X POST https://bert-fastapi-service-70236624058.europe-west1.run.app/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "I love Cloud Run!"}' | jq
```

RÃ©ponse attendue :
```bash
{
  "tweet": "I love Cloud Run!",
  "prediction": 4,
  "confidence": 0.89,
  "sentiment": "Tweet positif"
}
```

##Â ğŸ“š 3. PrÃ©diction sur plusieurs tweets (/predict_batch)

Ce endpoint accepte une liste de textes et renvoie une prÃ©diction pour chacun.

```bash
curl -X POST https://bert-fastapi-service-70236624058.europe-west1.run.app/predict_batch \
  -H "Content-Type: application/json" \
  -d '{
    "texts": [
      "I love Cloud Run!",
      "This is terrible.",
      "BERT is awesome!"
    ]
  }' | jq
  ```

RÃ©ponse attendue :
```bash
[
  {
    "tweet": "I love Cloud Run!",
    "prediction": 4,
    "confidence": 0.89,
    "sentiment": "Tweet positif"
  },
  {
    "tweet": "This is terrible.",
    "prediction": 0,
    "confidence": 0.92,
    "sentiment": "Tweet nÃ©gatif"
  },
  {
    "tweet": "BERT is awesome!",
    "prediction": 4,
    "confidence": 0.93,
    "sentiment": "Tweet positif"
  }
]
```
â„¹ï¸ Remarques

    Tous les endpoints acceptent et renvoient du JSON.

    Le modÃ¨le renvoie un score de prediction entre 0 (nÃ©gatif) et 4 (positif).

    La clÃ© confidence correspond Ã  la probabilitÃ© associÃ©e Ã  la prÃ©diction.








